{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hany5\\testing-app\\new_superpoint_transformer\n"
     ]
    }
   ],
   "source": [
    "%cd new_superpoint_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\envs\\spt\\lib\\site-packages\\pyntcloud\\utils\\numba.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def groupby_count(xyz, indices, out):\n",
      "d:\\Coding\\envs\\spt\\lib\\site-packages\\pyntcloud\\utils\\numba.py:12: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def groupby_sum(xyz, indices, N, out):\n",
      "d:\\Coding\\envs\\spt\\lib\\site-packages\\pyntcloud\\utils\\numba.py:19: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def groupby_max(xyz, indices, N, out):\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "from src.data import Data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_path = \"C:\\\\Users\\\\hany5\\\\Downloads\\\\test\\\\_vswork_room15.ply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_file = PyntCloud.from_file(ply_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_file.points['x'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = PyntCloud.from_file(ply_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyntCloud\n",
       "1844000 points with 5 scalar fields\n",
       "0 faces in mesh\n",
       "0 kdtrees\n",
       "0 voxelgrids\n",
       "Centroid: 9.349696159362793, -2.888927698135376, 1.6136233806610107\n",
       "Other attributes:"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ply(file_path):\n",
    "    \"\"\"\n",
    "    Read a .ply file and return a data object with the following fields:\n",
    "    - 'points': (N, 3) numpy array with the point coordinates\n",
    "    - 'colors': (N, 3) numpy array with the point colors\n",
    "    \"\"\"\n",
    "    pcd = PyntCloud.from_file(file_path)\n",
    "    \n",
    "    points = pcd.points\n",
    "    #Extract pos data\n",
    "    pos = points[['x', 'y', 'z']].values\n",
    "\n",
    "    #Extract color data\n",
    "    colors = points[['red', 'green', 'blue']].values\n",
    "    colors = colors / 255.0\n",
    "\n",
    "\n",
    "    data = Data()\n",
    "\n",
    "    data.pos = torch.tensor(pos, dtype=torch.float32)\n",
    "    data.rgb = torch.tensor(colors, dtype=torch.float32)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 261.,  -81.,   92.],\n",
      "        [ 275., -126.,    3.],\n",
      "        [ 275., -126.,    3.],\n",
      "        ...,\n",
      "        [ 355., -149.,   30.],\n",
      "        [ 356., -149.,   30.],\n",
      "        [ 356., -149.,   30.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from src.data import Data\n",
    "from src.utils import init_config\n",
    "from src.transforms import instantiate_datamodule_transforms\n",
    "\n",
    "# we use init_config to load the configuration file and do the exact same preprocessing as in the training pipeline\n",
    "cfg = init_config(overrides=[f\"experiment=semantic/s3dis\"])\n",
    "\n",
    "transforms_dict = instantiate_datamodule_transforms(cfg.datamodule)\n",
    "data = read_ply(ply_path)\n",
    "nag = transforms_dict['pre_transform'](data)\n",
    "from src.transforms import NAGRemoveKeys\n",
    "\n",
    "nag = NAGRemoveKeys(level=0, keys=[k for k in nag[0].keys if k not in cfg.datamodule.point_load_keys])(nag)\n",
    "nag = NAGRemoveKeys(level='1+', keys=[k for k in nag[1].keys if k not in cfg.datamodule.segment_load_keys])(nag)\n",
    "\n",
    "# Move to device\n",
    "nag = nag.cuda()\n",
    "\n",
    "# Apply on-device transforms\n",
    "nag = transforms_dict['on_device_test_transform'](nag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "nag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "\n",
    "ckpt_path = \"./checkpoints/last.ckpt\"\n",
    "# Instantiate the model and load pretrained weights\n",
    "model = hydra.utils.instantiate(cfg.model)\n",
    "model = model._load_from_checkpoint(ckpt_path)\n",
    "\n",
    "# Set the model in inference mode on the same device as the input\n",
    "model = model.eval().to(nag.device)\n",
    "\n",
    "# Inference, returns a task-specific ouput object carrying predictions\n",
    "with torch.no_grad():\n",
    "    output = model(nag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1749]), [64177, 1749, 441, 119])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.semantic_pred.shape, nag.num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the level-0 (voxel-wise) semantic segmentation predictions \n",
    "# based on the predictions on level-1 superpoints and save those for \n",
    "# visualization in the level-0 Data under the 'semantic_pred' attribute\n",
    "nag[0].semantic_pred = output.voxel_semantic_pred(super_index=nag[0].super_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.s3dis import CLASS_NAMES as S3DIS_CLASS_NAMES\n",
    "from src.datasets.s3dis import CLASS_COLORS as S3DIS_CLASS_COLORS\n",
    "\n",
    "nag.show(class_names=S3DIS_CLASS_NAMES, class_colors=S3DIS_CLASS_COLORS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
